\section{Optimization System}
\label{optimization}
This section will discuss the alternative proposal for arterial optimization, which was developed in response to DOGS.

\subsection{Coordination}
\label{coordination}
Coordination along an arterial is fundamental in signal optimization. The ideal situation for road users is the \textit{green wave}, where upon arrival to the next intersection there will always be a green light.

Contemporary car engines consume less fuel in general when they are allowed to run at a constant RPM level so the travel experience as well as emmission levels are improved under coordination.

There are also security aspects which indicate that coordination and green waves are desirable since the human eye has difficulty in observing acceleration and deceleration.

In one-way coordination a signal controller emits a platoon of vehicles over a period of time, say from $t_1$ to $t_2$, where $T_g = t_2 - t_1$ is indicative of the number of vehicles that need to pass. To avoid stopping these vehicles at the next signal the same amount of green time, $T_g$ must be given to the approaching platoon only \textit{offset} in time by $o$ time units to represent the travel time of the platoon from one signal to the next. 

In fact, due to \textit{platoon dispersion} SEE XXX XXX, which depend mainly on the intersignal distance and speeds, more than $T_g$ green time must be allotted to the stage of the downstream signal. It is also due to platoon dispersion that coordination is only relevant for signals which are relatively close. Practical experience from DRD indicate that the distance should not exceed 800-900 m. 

In perfect two-way coordination between two intersections the leading vehicles in platoons of traffic in either direction must experience that the next signal switch to green before they reach the area in which they decide to brake for red.

In a common cycle-time based system \cite{coord} gives us:

$$o = n \cdot \frac{C}{2}$$

Where $o$ is the travel time between the intersections, which is assumed to be the same in both directions, $C$ is the cycle time and $n$ is an integer.

The travel time is calculated as $o = L / v$ where $L$ is the distance and $v$ is the speed by which road users travel between the intersections. By insertion we get:

$$C = 2 \cdot \frac{L}{n \cdot v}$$

This equation is quite constrained, however. For cycle time we require that is lies within a span of about 60 til 120 seconds based on recommendations from DRD. If the cycle time is less stage lengths become too short to reach past the queue startup delays (about 4 vehicles), if more the minor roads experience long delays and pedestrians and bicyclists may start to cross the road before the signal is green.

In addition green waves usually span over more than two intersections with may have different distances and average speeds between them, all this making it hard or impossible to find an integer $n$. Thus green waves are most obvious in one, main direction. It is possible to affect the average speeds between intersections by changing the allowed speed-signs. If this is not sufficient one might accept that the green wave is not perfect by, for instance, prioritizing one direction over the other.

Priority can be given entirely to one direction or distributed by some weight, for instance based on the ratio of traffic in each direction. An obvious choice for the first option would be for an arterial with traffic between some suburbs and a major city. In the morning full priority should be given in the direction towards the city when people go to work and opposite in the afternoon. A third alternative is to a perfect progression band in one direction for a some ratio of time, corresponding to the ratios of traffic in either direction.
For ring road 3, which was simulated in this project, there is no clear distinction between the direction of morning and afternoon traffic (see Section \ref{data}) and a solution which weights the green wave quality according to direction ratio. 

The distance between one signal head (intersection) and the next can be extracted from the Vissim network file in both directions and is static. The signal heads for both directions of the arterial are marked using a naming convention similar to the one mentioned in Section \ref{routefractions} only less information is required since the relation of the head to its signal controller can be deduced directly from the network file. Further details on this process can be read in Section \ref{signal_details}.

For travel times (which is a key component in finding a proper offset) it is possible to rely on the speed limitations / free flow speeds for offset calculation, however under congestion speeds will decrease causing the travel times to increase. A better solution is to continously inspect the smoothened travel times which are inserted for each stretch. 

\subsection*{Why are we using common cycle schemes?}
In this section I highlight the restrictions imposed when introducing two-way coordination between common cycle time signal controllers. Are there no alternatives, which are less strict and why aren't they used in denmark?

With a common cycle time comes fixed signal plans that give us full insight into the operation of the each signal controller. We can see exactly which signal groups ie. heads are green, red, yellow and amber at any give second. Furthermore, as presented above, there exist expressions for the offset and cycle time which causes coordination. 

Although traffic actuated green time extension schemes exist to make fine adjustsmens, such as the ones used for bus priority, it is always necessary to take the seconds from some other stage due to the common cycle time requirement. Likewise with stage skipping, you must always "spend" $C$ cycle seconds and may never spend more.

These restrictions makes it difficult for a computer system to obtain optimum performance for the signal controllers and the observed / predicted traffic.

Stage based signals is an alternative to fixed signal plans where stages are queued for future execution along with a green time. There is no fixed order of the stages and no fixed green time. This gives great flexibility for the computer system, however the problem is the plans cannot be written down in advance in terms of $C$ for human inspection and control. Instead they must be defined dynamically on the basis of a set of signal rules, which are implemented and respected by the computer system while adapting to the measured traffic conditions. 

Here are some rules such a system should implement:

\begin{description}
\item[Coordination]
Whenever the arterial stage for a signal is active from $t_1$ to $t_2$ the arterial stage of the downstream signal should be active from $t_1 + o$ to $t_2 + o$ where $o$ is the travel time from signal to signal 2.
\item[Priority for the arterial]
The green times for all stages in the direction of the arterial must match the load on the arterial, respecting other rules.
\item[Fairness to minor roads]
The queues for minor roads cannot exceeed a predefined length nor must a vehicle wait more than a certain amount of time at the intersection to pass.
\end{description}

These rules - given proper parameters - are precise enough to be implemented in a computer program. The setting of parameters (for instance maximum queue length for minor roads) is a difficult subject, which could generate many training hours for end-users. This was my impression about the Utopia/Spot system the municipality of Copenhagen (Vej \& Park) had implemented for Centrum Forbindelsen - they had no idea on how to make adjustsments, when the system acted up. 
However, many parameters can - and should - be set automatically by repeated simulation with trial-and-error or they are either easy enough to understand that rules of thumb and area specific insight is enough to make proper choices.

I believe stage based signals and more centralized computer control of arterials could be accepted and achieved if systems were more available and open as proposed above. The introduction of stage based signals would be a major break with tradition in denmark - and probably also in many other countries. 

Breaking tradition is always hard; this combined with the volume of such a system has compelled me to remain in the common cycle time world in this project. But as will be seen in the following sections, there are many ways to improve arterial signal performance in spite of the the restrictions of $C$.

\subsection{Manipulating speed}
The first option, which is implemented in the search procedure (see section \ref{siman}), is to improve coordination by affecting the speeds of road users. This is done by adjusting speed signs since most motorists will obey these. The travel time between intersections, which largely determines the offset, could then be increased or decreased by changing the speed signs. 

In Figure \ref{fig:change_speed} we can see how this works. In the \textit{before} situation, platoons from intersection $i+1$ arrive too late and some experience red light. In the \textit{after} situation, the platoon is allowed to go faster and arrive entirely during the green time of intersection $i$.

\begin{figure}[ht]
\begin{center}
\includegraphics[scale=0.2]{change_speed.png} 
\end{center}
\caption{Changing speeds to obtain better coordination}
\label{fig:change_speed}
\end{figure}

Some considerations should be made in this respect. For instance we must ensure that the speeds do not divert too much from the norm of the relevant type of road. In addition the number of speed changes, which a motorist travelling throughout the arterial experience, must be minimized. And if speed changes do occur, it is a good idea to keep them small so that the speeds don't go from 50 to 70 from one stretch of road to the next.

From a security perspective it might seem risky to not persist a common speed level through the artery. But considering that a change of speed might improve the quality of the coordination, this problem is negated. This also applies to the additional acceleration and deceleration since, if a green wave does not exist the vehicles must be stopped altogether at the red light causing even more acceleration.

The current infrastructure on O3 does not offer this fine-grained level of adjustment but electronic speed signs are common practice nowadays and is, for instance, used on the almost-parallel motorring 3 to smoothen out queues.

\subsection{Adjusting offsets}
The individual signal offset is the traditional parameter for adjustment. The formulas presented earlier (section \ref{coordination}) can be used to establish one-way coordination, under the restrictions described, but two-way coordination is not solved so easily.

Basically the purpose of offsets is to properly align adjacent intersections in time such that a platoon emitted during the green time of intersection 1 is met by a green light at the adjacent intersection 2. The best estimate of the propagation time of the platoon is the travel time $tt_{1,2} = d / v$ where $v$ is the average speed and $d$ is the distance between the intersections and the green time displacement, $\Delta o_{i,j}$, should match $tt$ for all adjacent intersections $i$ and $j$.

\begin{figure}[ht]
\begin{center}
\includegraphics[scale=0.2]{change_offset.png} 
\end{center}
\caption{Changing speeds to obtain better coordination}
\label{fig:change_offset}
\end{figure}

We can change both the travel time, by changing speed signs, and also the green time displacement by adjusting offsets. The consequences of changing an offsets can be difficult to foresee, consider the example in Figure \ref{fig:changing_offset}. From intersection $j$ to $i$ the platoon will arrive too early. We can increase the offset (delay the green start) of $j$ but then we just reverse the situation. During all this, we must expect that there are also signal controllers $i-1$ and $j+1$ to the left and right resp. of $i$ and $j$, which must be in coordination with $i$ and $j$.

Metaheuristic search procedures are designed to cope with situations such as these where the immediate effects of a change cannot be foretold or enumerated.

\subsection{Metaheuristic Search}
DRD has traditionally used TRANSYT to obtain coordination. TRANSYT employs a genetic algorithm to finds its solutions but often it is necessary to manually adjust the offsets in order to obtain a good two-way coordination or to give preference to some direction. 

This manual process involves creating good two-way green waves by compromise, which could be either some sacrifice in the quality of the green waves. 

In this project the green wave concept has been formalized so that it is possible to evaluate a proposal for offsets. This opens up for the application of a metaheuristic search procedure.

In this first cycle-based approach the variable to be optimized is, of course, the offset for each signal controller. The signal controllers themselves operate under cyclic plans which are shifted in time according to the chosen offset. In addition the allowed speeds from a signal to the next may be changed to obtain better results.

\label{eval_coord}
For evaluation of a \textit{coordination} between $sc_1$ and $sc_2$ (directed) in the context of a time horizon $H = h_{min} .. h_{max}$ we examine each green band, which is emitted from $sc_1$. Given the distance and chosen speed from $sc_1$ to $sc_2$ we can count how many seconds worth of green band, which is not being let through by $sc_2$. The band from $sc_1$ may be cut in three ways, if $sc_2$ does not provide a green light, when the band reaches it 1) completely ie. there is a red light for the duration of the band 2) the front is cut off 3) the tail is cut off.

Obviously the first option is worst as all vehicles must halt. Second is the cutting of the leading vehicles, since they must brake or come to a halt, effectively halting the entire platoon. Least intrusive is the cutting of the tail, since then, at least, parts of the platoon actually experienced a green wave when travelling between $sc_1$ and $sc_2$.

\subsection{Simulated Annealing}
\label{siman}
In this section I describe the metaheuristic search routing, which was chosen: Simulated Annealing (SA). 
SA is a \textit{hill-climber} with the ability to escape from local optima ie. jump to another hill-top.

What this means is that SA performs a randomized, converging search in the entire search space (for offsets that is each combination of $N \bmod C$ over the intersections) looking for the possible best set of offsets without getting stuck with the \textit{first-and-best} solution it encounters.

The initial solution is chosen so that all offsets are zero. SA then works its way toward a better solution by examining neighbor solutions to the current one. A neighbor solution is found by incrementing or decrementing a single offset in the current solution or changing the allowed speed between the intersections by 5$km/h$.

The solutions to offsets are evaluated and compared in the context of the network and, if a neighbor is found to be better than the best solution found so far, it is adopted as the new \textit{encumbent}. If the neighbor was not better it should be thrown away, but here SA avoids being caught in a local optima by \textit{with some probability} keeping the neighbor solution regardless and work on from there. This probability will decrease as the search progress and so it is subject to tuning.

Generally metaheuristics give no guarantee that an optimum solution will be found, but with proper tuning and fast data structures, so that at least a couple of hundred solutions can be tested per second, it will generally yield good solutions. The strengths of metaheuristics are that they can easily support any kind of constraint, take advantage of the structure of the problem and run in O([insert seconds here]) ie. the running time is bounded by the time available.

In this project I have developed data structures, which are fast enough to generate the necessary iterations per second and can be used in various metaheuristic search schemes, not just simulated annealing.

The keys to speed in metaheuristic search are \textit{delta-evaluation} and to avoid \textit{object instantiation}.

\subsubsection{Delta-evaluation}
To evaluate a solution for offsets for sequentially adjacent signal controllers $sc_1$ to $sc_n$ we evaluate, in accordance to description in Section \ref{eval_coord}, each coordination $(sc_1,sc_2)$, $(sc_2,sc_1)$\footnote{Coordinations between adjacent signal controllers need not be symmetrical. For instance there might be a longer way in one direction that the other or, more likely, different \textit{speeds} might be allowed} and so forth. This gives us a value for each coordination and we then perform some kind of aggregation in order to obtain a single figure, the solution value.

Every time a neighbor solution is generated we normally immediately want to evaluate it as well. This can be done by going through the routine described above, but we can also use delta-evaluation to speed up the evaluation, if we have information on the structure of the problem.

A change in the offset of signal controller $sc_i$ will affect the coordinations between $sc_{i-1}$ and $sc_{i}$ and also between $sc_{i}$ and $sc_{i+1}$. Since we didn't change the offset $sc_{i-1}$ or $sc_{i+1}$ or any other offset, all coordinations but those mentioned will remain just as good - or bad - as before the offset change for $sc_i$. This is illustrated in Figure \ref{fig:delta_eval}.

\begin{figure}[!ht]
\begin{center}
\includegraphics[scale=0.3]{delta_eval.png} 
\end{center}
\caption{Local nature of coordinations. This illustrates the quality of coordinations between three signal controllers before and after changing the offset of the middle controller. Notice that the coordinations between the outer controllers and $sc_{i-2}$ and $sc_{i+2}$ remain the same.}
\label{fig:delta_eval}
\end{figure}

So, rather than evaluating every coordination again we may safely assume that only coordinations $(sc_{i-1},sc_{i})$, $(sc_{i-1},sc_{i})$ and $(sc_{i},sc_{i+1})$, $(sc_{i+1},sc_{i})$ need to be reevaluated. 

We now need only to evaluate at most four coordinations\footnote{Changing offsets for controllers in the ends of the arterial will only change the value of two coordinations.} to obtain the neighbor solution value. In total there are $2\cdot (n-1)$ coordinations to evaluate since each signal controller has an outgoing coordination in each direction except at the ends, which have only one. Thus we exchange a linearly growing reevaluation time (in the number of signal controllers) for a constant one and rough tests have shown that, even for small networks, the savings are in excess of a factor two, in spite of the extra overhead due to bookkeeping.

\subsubsection{Neighbor solutions}
In previous studies of metaheuristics, where the object oriented programming model was completely adopted, it was found that the overhead of object instantiation, whenever a neighbor solution was generated, would result in tremendous time consumption. 

Instead I have minimized the need for copying information from a solution to its neighbor by introducing the dual methods \verb|change| and \verb|undo_changes|. The purpose of these methods is to allow a solution to temporarily take the form of its neighbor and switch back to its former self, if requested. 
When the metaheuristic runs it will constantly ask for neighbor solutions but throw away most of them. This approach avoids much copying of solution attributes and at the same time the details of the switching of identify are relayed to the solution object.

\subsubsection{Bookkeeping outline}
The \verb|change| and \verb|undo_changes| methods are closely integrated with delta-evaluation. The initial solution obviously cannot be delta-evaluated so, during full evaluation, the \textit{contribution} from each coordination is noted. 

When \verb|change| is called a signal controller whose offset to change is chosen as well as a new offset value. Next we find the affected coordinations and update the value of the solution by swapping the old contributions with the new ones, when evaluated under the changed offset.

We must be ready to undo the changes we just made, so we track all these changes: which offset we changed and by how much as well as the previous contributions from the affected coordinations.

The extra time spent on bookkeeping is bounded by the number of affected coordinations, as explained previously and does increase the complexity of the codes by a great amount.


